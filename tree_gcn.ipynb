{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  origin finaldest  year  quarter  stops  avgprice  OriginLongitude  \\\n",
      "0    ABI       ABQ  2010        2      1    235.40          -99.682   \n",
      "1    ABI       ABQ  2010        3      1    180.75          -99.682   \n",
      "2    ABI       ABQ  2010        4      1    273.00          -99.682   \n",
      "3    ABI       ABQ  2011        1      1    223.00          -99.682   \n",
      "4    ABI       ABQ  2011        2      1    298.00          -99.682   \n",
      "\n",
      "   OriginLatitude  DestLongitude  DestLatitude  \n",
      "0          32.411       -106.609         35.04  \n",
      "1          32.411       -106.609         35.04  \n",
      "2          32.411       -106.609         35.04  \n",
      "3          32.411       -106.609         35.04  \n",
      "4          32.411       -106.609         35.04  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data = pd.read_csv('cleaned.csv', index_col=0)\n",
    "\n",
    "# For developing purpose\n",
    "# data = data.sample(n=2000, random_state=42)\n",
    "\n",
    "print(data.head())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['avgprice'])  \n",
    "y = data['avgprice'] \n",
    "\n",
    "X = data.drop(columns=['avgprice']) \n",
    "y = data['avgprice'] \n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "X_raw = pd.concat([X_train_raw, X_test_raw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (550463, 475), (550463,)\n",
      "Test set: (28972, 475), (28972,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = pd.get_dummies(X_raw, columns=['origin', 'finaldest'], drop_first=True)\n",
    "\n",
    "X_train = X.iloc[:len(X_train_raw)].reset_index(drop=True)\n",
    "X_test = X.iloc[len(X_train_raw):].reset_index(drop=True)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}, {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=42, n_estimators=10, max_depth=10)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = rf_model.predict(X_train)\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training MSE: {train_mse:.4f}\")\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n",
    "print(f\"Training MAE: {train_mae:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Tool\\Anaconda\\envs\\10703\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [01:08:58] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0ed59c031377d09b8-1\\xgboost\\xgboost-ci-windows\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 18795.0833\n",
      "Test MSE: 23950.8395\n",
      "Training MAE: 89.5970\n",
      "Test MAE: 95.6015\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(tree_method='hist', device='cuda', n_estimators=10, max_depth=10, random_state=42)\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = xgb_model.predict(X_train)\n",
    "y_test_pred = xgb_model.predict(X_test)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training MSE: {train_mse:.4f}\")\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n",
    "print(f\"Training MAE: {train_mae:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Simple GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, node_feature_dim, edge_feature_dim, global_feature_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(node_feature_dim, 128)\n",
    "        self.conv2 = GCNConv(128, 32)\n",
    "        self.conv3 = GCNConv(32, 8)\n",
    "        self.fc1 = torch.nn.Linear(8*2 + edge_feature_dim + global_feature_dim, 16)\n",
    "        self.fc2 = torch.nn.Linear(16, 1) \n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, year, quarter):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        \n",
    "        x = torch.cat([x[edge_index[0]], x[edge_index[1]], edge_attr, year, quarter], dim=1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool, global_max_pool\n",
    "\n",
    "class LargeGCN(torch.nn.Module):\n",
    "    def __init__(self, node_feature_dim, edge_feature_dim, global_feature_dim):\n",
    "        super(LargeGCN, self).__init__()\n",
    "\n",
    "        # GCN Layers (Wider feature sizes)\n",
    "        self.conv1 = GCNConv(node_feature_dim, 1024)  # Feature size: 1024\n",
    "        self.conv2 = GCNConv(1024, 512)              # Feature size: 512\n",
    "        self.conv3 = GCNConv(512, 256)               # Feature size: 256\n",
    "        self.conv4 = GCNConv(256, 128)               # Feature size: 128\n",
    "        self.conv5 = GCNConv(128, 64)                # Feature size: 64\n",
    "        \n",
    "        # Attention Mechanism (Enhances feature importance)\n",
    "        self.attn1 = GATConv(64, 64, heads=4, concat=True)\n",
    "        self.attn2 = GATConv(256, 64, heads=4, concat=True)\n",
    "\n",
    "        # Edge Features Transformation\n",
    "        self.edge_fc = torch.nn.Linear(edge_feature_dim, 128)\n",
    "\n",
    "        # Global Features Transformation\n",
    "        self.global_fc = torch.nn.Linear(global_feature_dim, 128)\n",
    "\n",
    "        # Graph Pooling Layers (Global Summary)\n",
    "        self.global_pool_mean = global_mean_pool\n",
    "        self.global_pool_max = global_max_pool\n",
    "\n",
    "        # Fully Connected Layers (Post-pooling)\n",
    "        self.fc1 = torch.nn.Linear(64 * 4 + 128 + 128, 512)\n",
    "        self.fc2 = torch.nn.Linear(512, 256)\n",
    "        self.fc3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "        # Regularization\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch, year, quarter):\n",
    "        # GCN Layers with ReLU activation\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "\n",
    "        # Attention Mechanism\n",
    "        x = F.relu(self.attn1(x, edge_index))\n",
    "        x = F.relu(self.attn2(x, edge_index))\n",
    "\n",
    "        # Edge features\n",
    "        edge_attr = F.relu(self.edge_fc(edge_attr))\n",
    "\n",
    "        # Global features\n",
    "        global_features = F.relu(self.global_fc(torch.cat([year, quarter], dim=1)))\n",
    "\n",
    "        # Pooling (Summarize the graph)\n",
    "        x_mean = self.global_pool_mean(x, batch)\n",
    "        x_max = self.global_pool_max(x, batch)\n",
    "        x = torch.cat([x_mean, x_max], dim=1)\n",
    "\n",
    "        # Concatenate with edge and global features\n",
    "        x = torch.cat([x, edge_attr, global_features], dim=1)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc3(x).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "airport_map = {airport: idx for idx, airport in enumerate(pd.concat([X_raw['origin'], X_raw['finaldest']]).unique())}\n",
    "X_raw['origin_idx'] = X_raw['origin'].map(airport_map)\n",
    "X_raw['finaldest_idx'] = X_raw['finaldest'].map(airport_map)\n",
    "\n",
    "X_train = X_raw.iloc[:len(X_train_raw)].reset_index(drop=True)\n",
    "X_test = X_raw.iloc[len(X_train_raw):].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index_train = torch.tensor(np.array([X_train['origin_idx'].values, X_train['finaldest_idx'].values]), dtype=torch.long)\n",
    "edge_index_test = torch.tensor(np.array([X_test['origin_idx'].values, X_test['finaldest_idx'].values]), dtype=torch.long)\n",
    "\n",
    "# for edge attribute, it currently only stores the stops.\n",
    "edge_attr_train = torch.tensor(X_train['stops'].values.reshape(-1, 1), dtype=torch.float)\n",
    "edge_attr_test = torch.tensor(X_test['stops'].values.reshape(-1, 1), dtype=torch.float)\n",
    "\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float)\n",
    "\n",
    "year_train = torch.tensor(X_train['year'].values, dtype=torch.float).reshape(-1, 1)\n",
    "quarter_train = torch.tensor(X_train['quarter'].values, dtype=torch.float).reshape(-1, 1)\n",
    "year_test = torch.tensor(X_test['year'].values, dtype=torch.float).reshape(-1, 1)\n",
    "quarter_test = torch.tensor(X_test['quarter'].values, dtype=torch.float).reshape(-1, 1)\n",
    "\n",
    "num_nodes = len(airport_map)\n",
    "node_features = torch.eye(num_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Node feature: onehot code + cooardinates\n",
    "\n",
    "coordinates = pd.DataFrame({\n",
    "    'airport': list(airport_map.keys()),\n",
    "    'longitude': [X_raw[X_raw['origin'] == airport]['OriginLongitude'].values[0] if airport in X_raw['origin'].values else X_raw[X_raw['finaldest'] == airport]['DestLongitude'].values[0] for airport in airport_map],\n",
    "    'latitude': [X_raw[X_raw['origin'] == airport]['OriginLatitude'].values[0] if airport in X_raw['origin'].values else X_raw[X_raw['finaldest'] == airport]['DestLatitude'].values[0] for airport in airport_map]\n",
    "})\n",
    "\n",
    "one_hot_nodes = torch.eye(len(airport_map))  \n",
    "coordinates_tensor = torch.tensor(coordinates[['longitude', 'latitude']].values, dtype=torch.float)\n",
    "node_features = torch.cat([one_hot_nodes, coordinates_tensor], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 105125.5938, Test Loss: 106192.2656\n",
      "Epoch 11/1000, Train Loss: 95084.8203, Test Loss: 96099.1094\n",
      "Epoch 21/1000, Train Loss: 85682.2344, Test Loss: 86768.3828\n",
      "Epoch 31/1000, Train Loss: 76260.6719, Test Loss: 77242.2578\n",
      "Epoch 41/1000, Train Loss: 66217.7344, Test Loss: 67111.7188\n",
      "Epoch 51/1000, Train Loss: 56021.6289, Test Loss: 56903.1875\n",
      "Epoch 61/1000, Train Loss: 46890.3516, Test Loss: 47923.1719\n",
      "Epoch 71/1000, Train Loss: 40782.2812, Test Loss: 42188.5391\n",
      "Epoch 81/1000, Train Loss: 39024.8633, Test Loss: 40770.9766\n",
      "Epoch 91/1000, Train Loss: 39080.5039, Test Loss: 40818.4414\n",
      "Epoch 101/1000, Train Loss: 38978.2109, Test Loss: 40727.6133\n",
      "Epoch 111/1000, Train Loss: 38930.7656, Test Loss: 40675.3984\n",
      "Epoch 121/1000, Train Loss: 38871.2891, Test Loss: 40613.0195\n",
      "Epoch 131/1000, Train Loss: 38810.8125, Test Loss: 40549.8984\n",
      "Epoch 141/1000, Train Loss: 38749.3594, Test Loss: 40486.3203\n",
      "Epoch 151/1000, Train Loss: 38693.7148, Test Loss: 40429.1953\n",
      "Epoch 161/1000, Train Loss: 38648.5898, Test Loss: 40383.4727\n",
      "Epoch 171/1000, Train Loss: 38609.1836, Test Loss: 40343.3633\n",
      "Epoch 181/1000, Train Loss: 38570.3906, Test Loss: 40303.2539\n",
      "Epoch 191/1000, Train Loss: 38530.3984, Test Loss: 40261.7539\n",
      "Epoch 201/1000, Train Loss: 38489.4727, Test Loss: 40221.5117\n",
      "Epoch 211/1000, Train Loss: 38468.6953, Test Loss: 40200.0312\n",
      "Epoch 221/1000, Train Loss: 38447.7461, Test Loss: 40178.3477\n",
      "Epoch 231/1000, Train Loss: 38426.6445, Test Loss: 40156.5000\n",
      "Epoch 241/1000, Train Loss: 38405.3984, Test Loss: 40134.5195\n",
      "Epoch 251/1000, Train Loss: 38383.9844, Test Loss: 40112.3945\n",
      "Epoch 261/1000, Train Loss: 38362.4180, Test Loss: 40090.0938\n",
      "Epoch 271/1000, Train Loss: 38340.6914, Test Loss: 40067.6016\n",
      "Epoch 281/1000, Train Loss: 38318.7891, Test Loss: 40044.9336\n",
      "Epoch 291/1000, Train Loss: 38296.6992, Test Loss: 40022.0820\n",
      "Epoch 301/1000, Train Loss: 38274.4180, Test Loss: 40000.1797\n",
      "Epoch 311/1000, Train Loss: 38263.1992, Test Loss: 39988.5820\n",
      "Epoch 321/1000, Train Loss: 38251.9453, Test Loss: 39976.9453\n",
      "Epoch 331/1000, Train Loss: 38240.6641, Test Loss: 39965.2773\n",
      "Epoch 341/1000, Train Loss: 38229.3359, Test Loss: 39953.5664\n",
      "Epoch 351/1000, Train Loss: 38217.9766, Test Loss: 39941.8242\n",
      "Epoch 361/1000, Train Loss: 38206.5820, Test Loss: 39930.0312\n",
      "Epoch 371/1000, Train Loss: 38195.1328, Test Loss: 39918.1953\n",
      "Epoch 381/1000, Train Loss: 38183.6406, Test Loss: 39906.3164\n",
      "Epoch 391/1000, Train Loss: 38172.0977, Test Loss: 39894.3828\n",
      "Epoch 401/1000, Train Loss: 38160.5039, Test Loss: 39882.9922\n",
      "Epoch 411/1000, Train Loss: 38154.6797, Test Loss: 39876.9805\n",
      "Epoch 421/1000, Train Loss: 38148.8555, Test Loss: 39870.9531\n",
      "Epoch 431/1000, Train Loss: 38143.0117, Test Loss: 39864.9141\n",
      "Epoch 441/1000, Train Loss: 38137.1602, Test Loss: 39858.8711\n",
      "Epoch 451/1000, Train Loss: 38131.2969, Test Loss: 39852.8086\n",
      "Epoch 461/1000, Train Loss: 38125.4219, Test Loss: 39846.7305\n",
      "Epoch 471/1000, Train Loss: 38119.5273, Test Loss: 39840.6328\n",
      "Epoch 481/1000, Train Loss: 38113.6211, Test Loss: 39834.5273\n",
      "Epoch 491/1000, Train Loss: 38107.6992, Test Loss: 39828.4023\n",
      "Epoch 501/1000, Train Loss: 38101.7578, Test Loss: 39822.5664\n",
      "Epoch 511/1000, Train Loss: 38098.7812, Test Loss: 39819.4883\n",
      "Epoch 521/1000, Train Loss: 38095.7969, Test Loss: 39816.4023\n",
      "Epoch 531/1000, Train Loss: 38092.8125, Test Loss: 39813.3086\n",
      "Epoch 541/1000, Train Loss: 38089.8164, Test Loss: 39810.2148\n",
      "Epoch 551/1000, Train Loss: 38086.8203, Test Loss: 39807.1172\n",
      "Epoch 561/1000, Train Loss: 38083.8203, Test Loss: 39804.0195\n",
      "Epoch 571/1000, Train Loss: 38080.8203, Test Loss: 39800.9102\n",
      "Epoch 581/1000, Train Loss: 38077.8086, Test Loss: 39797.7930\n",
      "Epoch 591/1000, Train Loss: 38074.7891, Test Loss: 39794.6797\n",
      "Epoch 601/1000, Train Loss: 38071.7656, Test Loss: 39791.7070\n",
      "Epoch 611/1000, Train Loss: 38070.2539, Test Loss: 39790.1406\n",
      "Epoch 621/1000, Train Loss: 38068.7383, Test Loss: 39788.5742\n",
      "Epoch 631/1000, Train Loss: 38067.2148, Test Loss: 39787.0000\n",
      "Epoch 641/1000, Train Loss: 38065.6992, Test Loss: 39785.4336\n",
      "Epoch 651/1000, Train Loss: 38064.1758, Test Loss: 39783.8555\n",
      "Epoch 661/1000, Train Loss: 38062.6523, Test Loss: 39782.2812\n",
      "Epoch 671/1000, Train Loss: 38061.1250, Test Loss: 39780.6992\n",
      "Epoch 681/1000, Train Loss: 38059.5977, Test Loss: 39779.1211\n",
      "Epoch 691/1000, Train Loss: 38058.0664, Test Loss: 39777.5312\n",
      "Epoch 701/1000, Train Loss: 38056.5352, Test Loss: 39776.0273\n",
      "Epoch 711/1000, Train Loss: 38055.7617, Test Loss: 39775.2266\n",
      "Epoch 721/1000, Train Loss: 38054.9922, Test Loss: 39774.4297\n",
      "Epoch 731/1000, Train Loss: 38054.2266, Test Loss: 39773.6367\n",
      "Epoch 741/1000, Train Loss: 38053.4531, Test Loss: 39772.8320\n",
      "Epoch 751/1000, Train Loss: 38052.6836, Test Loss: 39772.0352\n",
      "Epoch 761/1000, Train Loss: 38051.9062, Test Loss: 39771.2344\n",
      "Epoch 771/1000, Train Loss: 38051.1289, Test Loss: 39770.4297\n",
      "Epoch 781/1000, Train Loss: 38050.3516, Test Loss: 39769.6289\n",
      "Epoch 791/1000, Train Loss: 38049.5742, Test Loss: 39768.8164\n",
      "Epoch 801/1000, Train Loss: 38048.7930, Test Loss: 39768.0508\n",
      "Epoch 811/1000, Train Loss: 38048.4023, Test Loss: 39767.6523\n",
      "Epoch 821/1000, Train Loss: 38048.0117, Test Loss: 39767.2422\n",
      "Epoch 831/1000, Train Loss: 38047.6250, Test Loss: 39766.8359\n",
      "Epoch 841/1000, Train Loss: 38047.2305, Test Loss: 39766.4297\n",
      "Epoch 851/1000, Train Loss: 38046.8320, Test Loss: 39766.0273\n",
      "Epoch 861/1000, Train Loss: 38046.4414, Test Loss: 39765.6172\n",
      "Epoch 871/1000, Train Loss: 38046.0508, Test Loss: 39765.2070\n",
      "Epoch 881/1000, Train Loss: 38045.6602, Test Loss: 39764.8008\n",
      "Epoch 891/1000, Train Loss: 38045.2578, Test Loss: 39764.3867\n",
      "Epoch 901/1000, Train Loss: 38044.8672, Test Loss: 39764.0000\n",
      "Epoch 911/1000, Train Loss: 38044.6680, Test Loss: 39763.7930\n",
      "Epoch 921/1000, Train Loss: 38044.4688, Test Loss: 39763.5859\n",
      "Epoch 931/1000, Train Loss: 38044.2695, Test Loss: 39763.3828\n",
      "Epoch 941/1000, Train Loss: 38044.0703, Test Loss: 39763.1719\n",
      "Epoch 951/1000, Train Loss: 38043.8672, Test Loss: 39762.9609\n",
      "Epoch 961/1000, Train Loss: 38043.6719, Test Loss: 39762.7578\n",
      "Epoch 971/1000, Train Loss: 38043.4688, Test Loss: 39762.5469\n",
      "Epoch 981/1000, Train Loss: 38043.2695, Test Loss: 39762.3398\n",
      "Epoch 991/1000, Train Loss: 38043.0664, Test Loss: 39762.1328\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "node_feature_dim = node_features.shape[1] \n",
    "edge_feature_dim = edge_attr_train.shape[1]\n",
    "global_feature_dim = 2  # Year and quarter as two additional features\n",
    "model = GCN(node_feature_dim, edge_feature_dim, global_feature_dim)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to()\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(node_features, edge_index_train, edge_attr_train, year_train, quarter_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(node_features, edge_index_test, edge_attr_test, year_test, quarter_test)\n",
    "        loss = criterion(output, y_test)\n",
    "    return loss.item()\n",
    "\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train()\n",
    "    scheduler.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        test_loss = test()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10703",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
